{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s4kh4rov/BMIL/blob/master/Untitled5678.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5d-IUNu-b7Y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import xml.etree.ElementTree as ElementTree\n",
        "import os\n",
        "import math\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from matplotlib import patches\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def xml_to_dict(path_to_annot):\n",
        "    tree = ElementTree.parse(path_to_annot)\n",
        "    annotation = tree.getroot()\n",
        "\n",
        "    filename = annotation.find(\"filename\")\n",
        "    width = annotation.find(\"size\").find(\"width\")\n",
        "    height = annotation.find(\"size\").find(\"height\")\n",
        "\n",
        "    bndbox = annotation.find(\"object\").find(\"bndbox\")\n",
        "    xmin = bndbox.find(\"xmin\")\n",
        "    xmax = bndbox.find(\"xmax\")\n",
        "    ymin = bndbox.find(\"ymin\")\n",
        "    ymax = bndbox.find(\"ymax\")\n",
        "\n",
        "    label = 1\n",
        "\n",
        "    row = {\n",
        "        \"image_id\": filename.text.split('.')[0],\n",
        "        \"filename\": filename.text,\n",
        "        \"width\": width.text,\n",
        "        \"height\": height.text,\n",
        "        \"label\": label,\n",
        "        \"xmin\": xmin.text,\n",
        "        \"xmax\": xmax.text,\n",
        "        \"ymin\": ymin.text,\n",
        "        \"ymax\": ymax.text\n",
        "    }\n",
        "    return row"
      ],
      "metadata": {
        "id": "Vpcg3Dqn-6-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_csv_annot_file(path_to_annot_dir):\n",
        "    rows = []\n",
        "    annot_filenames = [join(path_to_annot_dir, f) for f in listdir(path_to_annot_dir) if\n",
        "                       isfile(join(path_to_annot_dir, f))]\n",
        "    for f in annot_filenames:\n",
        "        rows.append(xml_to_dict(f))\n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_csv('/content/drive/MyDrive/train.csv')"
      ],
      "metadata": {
        "id": "jgA_kEXy_A3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def view(images, labels, k, std=1, mean=0):\n",
        "    figure = plt.figure(figsize=(30, 30))\n",
        "    images = list(images)\n",
        "    labels = list(labels)\n",
        "    for i in range(k):\n",
        "        out = torchvision.utils.make_grid(images[i])\n",
        "        inp = out.cpu().numpy().transpose((1, 2, 0))\n",
        "        inp = np.array(std) * inp + np.array(mean)\n",
        "        inp = np.clip(inp, 0, 1)\n",
        "        ax = figure.add_subplot(2, 2, i + 1)\n",
        "        ax.imshow(images[i].cpu().numpy().transpose((1, 2, 0)))\n",
        "        l = labels[i]['boxes'].cpu().numpy()\n",
        "        l[:, 2] = l[:, 2] - l[:, 0]\n",
        "        l[:, 3] = l[:, 3] - l[:, 1]\n",
        "        for j in range(len(l)):\n",
        "            ax.add_patch(\n",
        "                patches.Rectangle((l[j][0], l[j][1]), l[j][2], l[j][3], linewidth=2, edgecolor='w', facecolor='none'))\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "EPT2QZvL_G6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SignboardDataset(Dataset):\n",
        "    def __init__(self, root, folder='train', transforms=None):\n",
        "        self.transforms = []\n",
        "        if transforms != None:\n",
        "            self.transforms.append(transforms)\n",
        "        self.root = root\n",
        "        self.folder = folder\n",
        "        self.box_data = pd.read_csv(os.path.join(root, \"train.csv\"))\n",
        "        self.imgs = list(os.listdir(os.path.join(root, self.folder)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(os.path.join(self.root, self.folder), self.imgs[idx])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        df = self.box_data[self.box_data['image_id'].astype(str) == self.imgs[idx].split('.')[0]]\n",
        "        if df.shape[0] != 0:\n",
        "            boxes = df[['xmin', 'ymin', 'xmax', 'ymax']].astype(float).values\n",
        "            labels = np.ones(len(boxes))\n",
        "        else:\n",
        "            boxes = np.asarray([[0, 0, 0, 0]])\n",
        "            labels = np.ones(len(boxes))\n",
        "        for i in self.transforms:\n",
        "            img = i(img)\n",
        "\n",
        "        targets = {}\n",
        "        targets['boxes'] = torch.from_numpy(boxes).double()\n",
        "        targets['labels'] = torch.from_numpy(labels).type(torch.int64)\n",
        "        print(targets['labels'])\n",
        "        #targets['id'] = self.imgs[idx].split('.')[0]\n",
        "        return img.double(), targets"
      ],
      "metadata": {
        "id": "ihV0TaYl_IvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, optimizer, loader, device, epoch):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    \n",
        "#     lr_scheduler = None\n",
        "#     if epoch == 0:\n",
        "#         warmup_factor = 1.0 / 1000 # do lr warmup\n",
        "#         warmup_iters = min(1000, len(loader) - 1)\n",
        "        \n",
        "#         lr_scheduler = optim.lr_scheduler.LinearLR(optimizer, start_factor = warmup_factor, total_iters=warmup_iters)\n",
        "    \n",
        "    all_losses = []\n",
        "    all_losses_dict = []\n",
        "    \n",
        "    for images, targets in tqdm(loader):\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: torch.tensor(v).to(device) for k, v in t.items()} for t in targets]\n",
        "        model = model.double()\n",
        "        loss_dict = model(images, targets) # the model computes the loss automatically if we pass in targets\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        loss_dict_append = {k: v.item() for k, v in loss_dict.items()}\n",
        "        loss_value = losses.item()\n",
        "        \n",
        "        all_losses.append(loss_value)\n",
        "        all_losses_dict.append(loss_dict_append)\n",
        "        \n",
        "        if not math.isfinite(loss_value):\n",
        "            print(f\"Loss is {loss_value}, stopping trainig\") # train if loss becomes infinity\n",
        "            print(loss_dict)\n",
        "            sys.exit(1)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "#         if lr_scheduler is not None:\n",
        "#             lr_scheduler.step() # \n",
        "        \n",
        "    all_losses_dict = pd.DataFrame(all_losses_dict) # for printing\n",
        "    print(\"Epoch {}, lr: {:.6f}, loss: {:.6f}, loss_classifier: {:.6f}, loss_box: {:.6f}, loss_rpn_box: {:.6f}, loss_object: {:.6f}\".format(\n",
        "        epoch, optimizer.param_groups[0]['lr'], np.mean(all_losses),\n",
        "        all_losses_dict['loss_classifier'].mean(),\n",
        "        all_losses_dict['loss_box_reg'].mean(),\n",
        "        all_losses_dict['loss_rpn_box_reg'].mean(),\n",
        "        all_losses_dict['loss_objectness'].mean()\n",
        "    ))"
      ],
      "metadata": {
        "id": "ZADhi8NOF9Xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # import torch\n",
        "    # torch.cuda.empty_cache()\n",
        "    # print(\"-----------------------------Начало работы---------------------------------------\")\n",
        "    # print(\"-----------------------------Создание файла с аннотациями---------------------------------------\")\n",
        "    # make_csv_annot_file(\"/content/drive/MyDrive/annot\")\n",
        "    print(\"-----------------------------Создание датасета---------------------------------------\")\n",
        "    root = \"/content/drive/MyDrive/lc/\"\n",
        "   \n",
        "    # torchvision_transform = transforms.Compose([\n",
        "    #     transforms.RandomHorizontalFlip(),\n",
        "    #     transforms.RandomGrayscale(), \n",
        "    #     transforms.ToTensor(), \n",
        "    # ])\n",
        "    dataset = SignboardDataset(root, 'train', transforms=torchvision.transforms.ToTensor())\n",
        "    # dataset = SignboardDataset(root, 'train', transforms=torchvision_transform)\n",
        "    # print(dataset[4])\n",
        "    print(\"-----------------------------Разделение на выборки и создание модели---------------------------------------\")\n",
        "    # разделение на тренировочную и тестовую выборку и создание Dataloaders для загрузки данных батчами:\n",
        "    # изменить параметры для разделения dataset_train и dataset_test batch_size\n",
        "    # torch.manual_seed(2)\n",
        "    indices = torch.randperm(len(dataset)).tolist()\n",
        "    dataset_train = torch.utils.data.Subset(dataset, indices[-440:])\n",
        "    dataset_test = torch.utils.data.Subset(dataset, indices[:-440])\n",
        "    data_loader_train = torch.utils.data.DataLoader(dataset_train,\n",
        "                                                    batch_size=4,\n",
        "                                                    shuffle=False,\n",
        "                                                    collate_fn=lambda x: list(zip(*x)))\n",
        "    data_loader_test = torch.utils.data.DataLoader(dataset_test,\n",
        "                                                   batch_size=4,\n",
        "                                                   shuffle=False,\n",
        "                                                   collate_fn=lambda x: list(zip(*x)))\n",
        "    #изображение с bbox\n",
        "    # images, labels = next(iter(data_loader_train))\n",
        "    # view(images, labels, 2)\n",
        "\n",
        "    # модель\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "    # device = torch.device('cpu')\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    num_classes = 2  # 1 class + background\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    model = model.to(device)\n",
        "\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = torch.optim.SGD(params, lr=0.005,\n",
        "                                momentum=0.9, weight_decay=0.0005)\n",
        "    criterion = nn.BCELoss(size_average=True)\n",
        "    \n",
        "\n",
        "    # lossesarr = []\n",
        "    print(\"-----------------------------Начало тренировки---------------------------------------\")\n",
        "    num_epochs=10\n",
        "\n",
        "    for e in range(num_epochs):\n",
        "    train_loss = 0.0\n",
        "    correct = 0\n",
        "    for data, labels in data_loader_train:\n",
        "        # Transfer Data to GPU if available\n",
        "        if torch.cuda.is_available():\n",
        "            data, labels = data.cuda(), labels.cuda()\n",
        "         \n",
        "        # Clear the gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Forward Pass\n",
        "        target = model(data)\n",
        "        # Find the Loss\n",
        "        loss = criterion(target,labels)\n",
        "        # Calculate gradients\n",
        "        loss.backward()\n",
        "        # Update Weights\n",
        "        optimizer.step()\n",
        "        # Calculate Loss\n",
        "        train_loss += loss.item()\n",
        "        correct += (target == labels).float().sum()\n",
        "\n",
        "    accuracy = 100 * correct / len(dataset_train)\n",
        "     \n",
        "    valid_loss = 0.0\n",
        "    correct_valid  = 0\n",
        "    model.eval()     # Optional when not using Model Specific layer\n",
        "    for data, labels in data_loader_test:\n",
        "        # Transfer Data to GPU if available\n",
        "        if torch.cuda.is_available():\n",
        "            data, labels = data.cuda(), labels.cuda()\n",
        "         \n",
        "        # Forward Pass\n",
        "        target = model(data)\n",
        "        # Find the Loss\n",
        "        loss = criterion(target,labels)\n",
        "        # Calculate Loss\n",
        "        valid_loss += loss.item()\n",
        "        correct_valid += (target == labels).float().sum()\n",
        "    valid_accuracy = 100 * correct_valid / len(dataset_test)\n",
        "\n",
        "    print(\"Accuracy train = {}\".format(accuracy))\n",
        "    print(\"Accuracy valid = {}\".format(valid_accuracy))\n",
        "    print(f'Epoch {e+1} \\t\\t Training Loss: {\\\n",
        "    train_loss / len(trainloader)} \\t\\t Validation Loss: {\\\n",
        "    valid_loss / len(validloader)}')\n",
        "     \n",
        "    if min_valid_loss > valid_loss:\n",
        "        print(f'Validation Loss Decreased({min_valid_loss:.6f\\\n",
        "        }--->{valid_loss:.6f}) \\t Saving The Model')\n",
        "        min_valid_loss = valid_loss\n",
        "         \n",
        "        # Saving State Dict\n",
        "        torch.save(model.state_dict(), 'saved_model.pth')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # for epoch in range(num_epochs):\n",
        "    #     train_one_epoch(model, optimizer, data_loader_train, device, epoch)\n",
        "    # print(\"-----------------------------Сохранение модели---------------------------------------\")\n",
        "    # torch.save(model.state_dict(), '/content/drive/MyDrive/model6.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1f26TWI_bTK",
        "outputId": "ec7fe208-0ad1-4343-e714-29f5de9c5ad5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------Создание датасета---------------------------------------\n",
            "-----------------------------Разделение на выборки и создание модели---------------------------------------\n",
            "-----------------------------Начало тренировки---------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/110 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 1, 1])\n",
            "tensor([1])\n",
            "tensor([1])\n",
            "tensor([1])\n"
          ]
        }
      ]
    }
  ]
}