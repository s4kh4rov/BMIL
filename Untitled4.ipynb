{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "mount_file_id": "1LboYI3_4iMXAQvoesQF9JeTSsmS9goHF",
      "authorship_tag": "ABX9TyOiCUhYZhmRICZVh8kHSDZy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s4kh4rov/BMIL/blob/master/Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "d5d-IUNu-b7Y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import xml.etree.ElementTree as ElementTree\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from matplotlib import patches\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def xml_to_dict(path_to_annot):\n",
        "    tree = ElementTree.parse(path_to_annot)\n",
        "    annotation = tree.getroot()\n",
        "\n",
        "    filename = annotation.find(\"filename\")\n",
        "    width = annotation.find(\"size\").find(\"width\")\n",
        "    height = annotation.find(\"size\").find(\"height\")\n",
        "\n",
        "    bndbox = annotation.find(\"object\").find(\"bndbox\")\n",
        "    xmin = bndbox.find(\"xmin\")\n",
        "    xmax = bndbox.find(\"xmax\")\n",
        "    ymin = bndbox.find(\"ymin\")\n",
        "    ymax = bndbox.find(\"ymax\")\n",
        "\n",
        "    # 1 - вывеска\n",
        "    if filename.text.startswith(\"Domin\"):\n",
        "      label = 1\n",
        "    else:\n",
        "      label = 2;\n",
        "\n",
        "    row = {\n",
        "        \"image_id\": filename.text.split('.')[0],\n",
        "        \"filename\": filename.text,\n",
        "        \"width\": width.text,\n",
        "        \"height\": height.text,\n",
        "        \"label\": label,\n",
        "        \"xmin\": xmin.text,\n",
        "        \"xmax\": xmax.text,\n",
        "        \"ymin\": ymin.text,\n",
        "        \"ymax\": ymax.text\n",
        "    }\n",
        "    return row"
      ],
      "metadata": {
        "id": "Vpcg3Dqn-6-g"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_csv_annot_file(path_to_annot_dir):\n",
        "    rows = []\n",
        "    annot_filenames = [join(path_to_annot_dir, f) for f in listdir(path_to_annot_dir) if\n",
        "                       isfile(join(path_to_annot_dir, f))]\n",
        "    for f in annot_filenames:\n",
        "        rows.append(xml_to_dict(f))\n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_csv('/content/drive/MyDrive/train.csv')"
      ],
      "metadata": {
        "id": "jgA_kEXy_A3w"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def view(images, labels, k, std=1, mean=0):\n",
        "    figure = plt.figure(figsize=(30, 30))\n",
        "    images = list(images)\n",
        "    labels = list(labels)\n",
        "    for i in range(k):\n",
        "        out = torchvision.utils.make_grid(images[i])\n",
        "        inp = out.cpu().numpy().transpose((1, 2, 0))\n",
        "        inp = np.array(std) * inp + np.array(mean)\n",
        "        inp = np.clip(inp, 0, 1)\n",
        "        ax = figure.add_subplot(2, 2, i + 1)\n",
        "        ax.imshow(images[i].cpu().numpy().transpose((1, 2, 0)))\n",
        "        l = labels[i]['boxes'].cpu().numpy()\n",
        "        l[:, 2] = l[:, 2] - l[:, 0]\n",
        "        l[:, 3] = l[:, 3] - l[:, 1]\n",
        "        for j in range(len(l)):\n",
        "            ax.add_patch(\n",
        "                patches.Rectangle((l[j][0], l[j][1]), l[j][2], l[j][3], linewidth=2, edgecolor='w', facecolor='none'))\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "EPT2QZvL_G6F"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SignboardDataset(Dataset):\n",
        "    def __init__(self, root, folder='train', transforms=None):\n",
        "        self.transforms = []\n",
        "        if transforms != None:\n",
        "            self.transforms.append(transforms)\n",
        "        self.root = root\n",
        "        self.folder = folder\n",
        "        self.box_data = pd.read_csv(os.path.join(root, \"train.csv\"))\n",
        "        # print(\"Данные из train.csv в методе init\", self.box_data)\n",
        "        # self.box_data = pd.concat(\n",
        "        #     [box_data, box_data.bbox.str.split('[').str.get(1).str.split(']').str.get(0).str.split(',', expand=True)],\n",
        "        #     axis=1)\n",
        "        self.imgs = list(os.listdir(os.path.join(root, self.folder)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(os.path.join(self.root, self.folder), self.imgs[idx])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        df = self.box_data[self.box_data['image_id'] == self.imgs[idx].split('.')[0]]\n",
        "        # print(\"df в методе getItem \\n\", df)\n",
        "        print(\"idx\",idx)\n",
        "        if df.shape[0] != 0:\n",
        "            # df[2] = df[0].astype(float) + df[2].astype(float)\n",
        "            # df[3] = df[1].astype(float) + df[3].astype(float)\n",
        "            boxes = df[['xmin', 'ymin', 'xmax', 'ymax']].astype(float).values\n",
        "            # print(\"xmin0 \\n\",df[['xmin'][0]])\n",
        "            # boxes = np.asarray([[df[['xmin'][idx]],\n",
        "            #                      df[['ymin'][idx]],\n",
        "            #                      df[['xmax'][idx]],\n",
        "            #                      df[['ymax'][idx]]]])\n",
        "            labels = df['label'].astype(int).values\n",
        "            print(\"labels\",labels)\n",
        "        else:\n",
        "            boxes = np.asarray([[0, 0, 0, 0]])\n",
        "            labels = np.ones(len(boxes))\n",
        "        for i in self.transforms:\n",
        "            img = i(img)\n",
        "            # print(img.double())\n",
        "\n",
        "        targets = {}\n",
        "        targets['boxes'] = torch.from_numpy(boxes).double()\n",
        "        targets['labels'] = torch.from_numpy(labels).type(torch.int64)\n",
        "        #targets['id'] = self.imgs[idx].split('.')[0]\n",
        "        return img.double(), targets"
      ],
      "metadata": {
        "id": "ihV0TaYl_IvJ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # print(\"-----------------------------Начало работы---------------------------------------\")\n",
        "    # print(\"-----------------------------Создание файла с аннотациями---------------------------------------\")\n",
        "    # make_csv_annot_file(\"/content/drive/MyDrive/annot\")\n",
        "    print(\"-----------------------------Создание датасета---------------------------------------\")\n",
        "    root = \"/content/drive/MyDrive/\"\n",
        "    dataset = SignboardDataset(root, 'train', transforms=torchvision.transforms.ToTensor())\n",
        "    # print(dataset[4])\n",
        "    print(\"-----------------------------Разделение на выборки и создание модели---------------------------------------\")\n",
        "    # разделение на тренировочную и тестовую выборку и создание Dataloaders для загрузки данных батчами:\n",
        "    # изменить параметры для разделения dataset_train и dataset_test batch_size\n",
        "    torch.manual_seed(2)\n",
        "    indices = torch.randperm(len(dataset)).tolist()\n",
        "    dataset_train = torch.utils.data.Subset(dataset, indices[:-2100])\n",
        "    dataset_test = torch.utils.data.Subset(dataset, indices[-1000:])\n",
        "    data_loader_train = torch.utils.data.DataLoader(dataset_train,\n",
        "                                                    batch_size=1,\n",
        "                                                    shuffle=True,\n",
        "                                                    collate_fn=lambda x: list(zip(*x)))\n",
        "    data_loader_test = torch.utils.data.DataLoader(dataset_test,\n",
        "                                                   batch_size=4,\n",
        "                                                   shuffle=False,\n",
        "                                                   collate_fn=lambda x: list(zip(*x)))\n",
        "    #изображение с bbox\n",
        "    # images, labels = next(iter(data_loader_train))\n",
        "    # view(images, labels, 2)\n",
        "\n",
        "    # модель\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    num_classes = 2  # 1 class + background\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    model = model.to(device)\n",
        "\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = torch.optim.SGD(params, lr=0.01)\n",
        "\n",
        "    lossesarr = []\n",
        "    #тренировка\n",
        "    print(\"-----------------------------Начало тренировки---------------------------------------\")\n",
        "    model.train()\n",
        "\n",
        "    for epoch in tqdm(range(1)):\n",
        "        for images, targets in tqdm(data_loader_train):\n",
        "            images = list(image.to(device) for image in images)\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "            model = model.double()\n",
        "            loss_dict = model(images, targets)\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "            losses.backward()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            optimizer.step()\n",
        "        lossesarr.append(losses)\n",
        "        print(\"Loss = {:.4f} \".format(losses.item()))\n",
        "\n",
        "    print(\"-----------------------------Сохранение модели---------------------------------------\")\n",
        "    torch.save(model.state_dict(), '/content/drive/MyDrive/model2.pth')\n",
        "\n",
        "    plt.plot(lossesarr)\n",
        "    plt.title('Loss vs Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('loss')\n",
        "\n",
        "    # model.load_state_dict(torch.load('/content/drive/MyDrive/model.pth'))\n",
        "    \n",
        "    # images, targets = next(iter(data_loader_test))\n",
        "    # images = list(image.to(device) for image in images)\n",
        "    # targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "    \n",
        "    # model.eval()\n",
        "    # output = model.double()(images)\n",
        "    \n",
        "    # with torch.no_grad():\n",
        "    #     view(images, output, 4)\n",
        "\n",
        "\n",
        "    # model.eval()\n",
        "    # torch.cuda.empty_cache()\n",
        "    # test_dataset = SignboardDataset(root, 'train', transforms=torchvision.transforms.ToTensor())\n",
        "    # img, _ = test_dataset[1500]\n",
        "    # img_int = torch.tensor(img*255, dtype=torch.uint8)\n",
        "    # with torch.no_grad():\n",
        "    #     prediction = model.double()([img.to(device)])\n",
        "    #     pred = prediction[0]\n",
        "    # fig = plt.figure(figsize=(14, 10))\n",
        "    # plt.imshow(draw_bounding_boxes(img_int,\n",
        "    #     pred['boxes'][pred['scores'] > 0.8],\n",
        "    #     [classes[i] for i in pred['labels'][pred['scores'] > 0.8].tolist()], width=4\n",
        "    # ).permute(1, 2, 0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "l1f26TWI_bTK",
        "outputId": "26f633d0-2ca9-4d5d-e599-e2f41691416f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------Создание датасета---------------------------------------\n",
            "-----------------------------Разделение на выборки и создание модели---------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-87cce3883ba7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi_heads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_predictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFastRCNNPredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    905\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 907\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m     def register_backward_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    903\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    904\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 905\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
          ]
        }
      ]
    }
  ]
}